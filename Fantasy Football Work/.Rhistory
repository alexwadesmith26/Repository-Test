threshold <- qbinom(0.05, ns[i], null_theta)
for (j in seq_along(theta_range)) {
power_curve[j] <- pbinom(threshold, size = ns[i], prob = theta_range[j])
}
if (i == 1) {
plot(theta_range, power_curve, col = plot_colors[[i]], xlab = "Theta", ylab = "Power", lwd = 5, type = "l", main = "Power curve (sample size increasing)",cex.lab=1.3)
} else {
points(theta_range, power_curve, col = plot_colors[[i]], lwd = 5, type = "l")
}
}
n <- 40
levels <- c(0.01, 0.05, 0.2)
for (i in seq_along(ns)) {
threshold <- qbinom(levels[i], n, null_theta)
for (j in seq_along(theta_range)) {
power_curve[j] <- pbinom(threshold, size = n, prob = theta_range[j])
}
if (i == 1) {
plot(theta_range, power_curve, col = plot_colors[[i]], xlab = "Theta", ylab = "Power", lwd = 5, type = "l", main = "Power curve (level changing)",cex.lab=1.3)
} else {
points(theta_range, power_curve, col = plot_colors[[i]], lwd = 5, type = "l")
}
}
ns <- c(40, 1000, 10000)
null_theta <- 0.5
theta_range <- seq(0.1, 0.5, 0.01)
power_curve <- rep(0, length(theta_range))
plot_colors <- list("red", "green", "blue")
for (i in seq_along(ns)) {
threshold <- qbinom(0.05, ns[i], null_theta)
for (j in seq_along(theta_range)) {
power_curve[j] <- pbinom(threshold, size = ns[i], prob = theta_range[j])
}
if (i == 1) {
plot(theta_range, power_curve, col = plot_colors[[i]], xlab = "Theta", ylab = "Power", lwd = 5, type = "l", main = "Power curve (sample size increasing)",cex.lab=1.3)
} else {
points(theta_range, power_curve, col = plot_colors[[i]], lwd = 5, type = "l")
}
}
source("plot_tails.R") # Also available in the course page
level <- 0.05
n <- 5
plot.dist(level, alternative = "two.tailed", distribution = "t", df = n - 1, main_title = paste("T(", n - 1, ")", sep = ""))
cat(sprintf("Critical region: t <= %.2f, t >= %.2f\n", qt(level / 2, n - 1), qt(level / 2, n - 1, lower.tail = FALSE)))
load("twin_data.RDa") # Data adapted from https://arxiv.org/pdf/0707.3794.pdf
twin_table <- table(twin_data$`Twin 1 Alcohol dependence`, twin_data$`Twin 2 Depression`)
chisq.test(twin_table)
twin_table
twin_data
chisq.test(twin_table)
twin_table <- table(twin_data$`Twin 2 Alcohol dependence`, twin_data$`Twin 1 Depression`)
chisq.test(twin_table)
twin_table <- table(twin_data$`Twin 1 Alcohol dependence`, twin_data$`Twin 1 Depression`)
chisq.test(twin_table)
twin_table <- table(twin_data$`Twin 1 Alcohol dependence`, twin_data$`Twin 2 Depression`)
chisq.test(twin_table)
twin_table <- table(twin_data$`Twin 2 Alcohol dependence`, twin_data$`Twin 1 Depression`)
chisq.test(twin_table)
plot.dist <- function(alpha, from = -5, to = 5, n = 1000, filename = NULL,
alternative = c("two.tailed", "greater", "lesser"),
distribution = c("normal", "t", "F", "chisq", "binomial"),
colour = "black", fill = "skyblue2", main_title = NULL, ...) {
alternative <- match.arg(alternative)
## Calculate alpha level given hypothesis
alt.alpha <- switch(alternative,
two.tailed = alpha/2,
greater = alpha,
lesser = alpha)
## use a 'switch' to pick the right functions based on distribution
my.den <- switch(distribution,
normal = dnorm,
t = dt,
F = df,
chisq = dchisq,
binomial = dbinom)
my.dist <- switch(distribution,
normal = qnorm,
t = qt,
F = qf,
chisq = qchisq,
binomial = qbinom)
## Additional arguments passed via '...' e.g., degrees of freedom
crit.lower <- my.dist(p = alt.alpha, lower.tail = TRUE, ...)
crit.upper <- my.dist(p = alt.alpha, lower.tail = FALSE, ...)
## Calculate alpha (lower) region coordinates
cord.x1 <- c(from, seq(from = from, to = crit.lower,
length.out = 100), crit.lower)
cord.y1 <- c(0, my.den(x = seq(from = from, to = crit.lower,
length.out = 100), ...), 0)
## Calculate alpha (upper) region coordinates
cord.x2 <- c(crit.upper, seq(from = crit.upper, to = to,
length.out = 100), to)
cord.y2 <- c(0, my.den(x = seq(from = crit.upper, to = to,
length.out = 100), ...), 0)
## Logic test to choose which graphic device to open
if(is.null(filename)) {
dev.new()
} else {
pdf(file = filename)
}
## plot distribution
curve(my.den(x, ...), from = from, to = to,
n = n, col = colour, lty = 1, lwd = 2,
ylab = "Density", xlab = "Values", main = main_title)
## Add alpha region(s) based on given hypothesis
if(!identical(alternative, "greater")) {
polygon(x = cord.x1, y = cord.y1, col = fill)
}
if(!identical(alternative, "lesser")) {
polygon(x = cord.x2, y = cord.y2, col = fill)
}
## If the PDF device was started, shut it down
if(!is.null(filename)) {dev.off()}
}
lambda_0 <- 6
n <- 20
alpha <- 0.05
q_alpha_bottom <- qpois(alpha/2, lambda_0, lower.tail=TRUE) # Fill this with the correct alpha / 2 quantile!
q_alpha_bottom
q_alpha_up     <- qpois(alpha/2, lambda_0, lower.tail=FALSE) # Fill this with the correct 1 - 1alpha / 2 quantile!
q_alpha_up
num_trials <- 10000 # Number of repeats
num_reject <- 0
for (i in seq_len(num_trials)) {
x <- rpois(n, lambda_0)
w <- (mean(x) - lambda_0) / sqrt(var(x) / n)
if (w < q_alpha_bottom || w > q_alpha_up) {
num_reject <- num_reject + 1
}
}
type_I_freq <- num_reject / num_trials
cat(sprintf("Advertised Type I error: %.2f\n", alpha))
cat(sprintf("Type I error frequency (%d trials): %.2f\n", num_trials, type_I_freq))
n <- 50 ## size of the data set
num_rep <- 20 ## number of repetitions
m <- rep(0, num_rep)
true_mean <- 168 ### true mean of the population's height
true_var <- 103 ### true variance of the populations's height
for (i in 1:num_rep) {
dat <- rnorm(n, mean = true_mean, sd = sqrt(true_var)) ## simulate n normal random variables with mean
# true_mean and variance true_var
m[i] <- mean(dat) ## compute the mean of the simulated dataset
}
library(ggplot2) # Available from CRAN
library(gridExtra)
df_sim <- data.frame(x = 1:num_rep, y = m)
df_sim
g <- ggplot(df_sim, aes(x = reorder(rownames(df_sim), 1:num_rep), y = y)) +
geom_point(size = 2) + geom_hline(yintercept = true_mean) +
coord_flip() + xlab("Dataset") + ylab(expression(hat(mu))) + ylim(164, 171) +
theme(axis.title.x = element_text(face = "bold", size = 20), axis.title.y = element_text(face = "bold", size = 20))
grid.arrange(g, ncol = 1)
n <- 50 ## number of samples
num_rep <- 20 ## number of repetitions of the experiment
m <- rep(0, num_rep)
L <- rep(0, num_rep)
true_mean <- 168 ## the population's mean height
true_var <- 103 ## the variance in the population's height
v <- qnorm(0.93) ## 0.93 quantile of a normal distribution
for (i in 1:num_rep) {
dat <- rnorm(n, mean = true_mean, sd = sqrt(true_var))
m[i] <- mean(dat)
}
L <- m - v * sqrt(true_var/ n)
U <- rep(Inf, num_rep)
df_sim <- data.frame(x = 1:num_rep, y = m, U = U, L = L)
g <- ggplot(df_sim, aes(x = reorder(rownames(df_sim), 1:num_rep), y = y)) +
geom_point(size = 2) + geom_hline(yintercept = true_mean) +
geom_errorbar(aes(ymax = U, ymin = L)) + ylim(162, 171) +
coord_flip() + xlab("Dataset") + ylab(expression(hat(mu))) +
theme(axis.title.x = element_text(face = "bold", size = 20), axis.title.y = element_text(face = "bold", size = 20))
grid.arrange(g, ncol = 1)
v <- qnorm(0.975)
U <- m + v * sqrt(true_var / n)
L <- m - v * sqrt(true_var / n) # qnorm(0.025) is the same as -qnorm(0.975)
df_sim <- data.frame(x = 1:num_rep, y = m, U = U, L = L)
g <- ggplot(df_sim, aes(x = reorder(rownames(df_sim), 1:num_rep), y = y)) +
geom_point(size = 2) + geom_hline(yintercept = true_mean) +
geom_errorbar(aes(ymin = L, ymax = U)) +
coord_flip() + xlab("Dataset") + ylab(expression(hat(mu))) +
theme(axis.title.x = element_text(face = "bold", size = 20), axis.title.y = element_text(face = "bold", size = 20))
grid.arrange(g, ncol = 1)
x <- matrix(UKgas) # Let's pretend this didn't change over time
x
n <- length(x)
h <- hist(x, plot = FALSE)
h
show(h)
print(h)
scale_x <- var(x) / mean(x)
shape_x <- mean(x) / scale_x
y <- dgamma(0:max(x), shape = shape_x, scale = scale_x)
plot(h, xlab = "millions of therms", main = "UK Gas consumption (1960-1986)")
lines(x = 0:max(x), y = y * length(x) * diff(h$breaks)[1], lwd = 5, col = "blue")
num_rep <- 200
x_bar <- rep(0, num_rep)
for (i in 1:num_rep) {
x_bar[i] <- mean(rgamma(n, shape = shape_x, scale = scale_x))
}
par(mfrow = c(1, 2))
hist(x_bar, xlab = expression(bar(X)), main = "Histogram")
qqnorm(x_bar, xlab = expression(bar(X)))
qqline(x_bar)
cat(sprintf("Normality test: %.2f\n", shapiro.test(x_bar)$p.value))
num_rep <- 200
x_bar <- rep(0, num_rep)
for (i in 1:num_rep) {
x_bar[i] <- mean(rgamma(n, shape = shape_x, scale = scale_x))
}
par(mfrow = c(1, 2))
hist(x_bar, xlab = expression(bar(X)), main = "Histogram")
qqnorm(x_bar, xlab = expression(bar(X)))
qqline(x_bar)
cat(sprintf("Normality test: %.2f\n", shapiro.test(x_bar)$p.value))
setwd("~/labs/Repository-Test/Fantasy Football Work")
m1 <- lm(adv$Sales ~ adv$TV)
mids_fwds.data <- read.csv("midfielders_forwards_data.csv")
head(mids_fwds.data)
columns(mids_fwds.data)
names(mids_fwds.data)
#filter data
X <- mids_fwds.data[!c(drop_vars)]
#specify final predictors for modelling
drop_vars <- c("start_time", "player_id", "fix_id", "points", "game_diff"
, "team_qual", "position_Defender", "position_Midfielder"
, "X3G_avg_minutes", "X3G_avg_bonus", "X3G_avg_goals_scored"
, "X3G_avg_assists", "X3G_avg_goals_scored_qual", "X3G_avg_assists_qual")
#filter data
X <- mids_fwds.data[!c(drop_vars)]
#filter data
X <- mids_fwds.data[, !names(mids_fwds.data) %in% drop_vars]
head(X)
#set response for regression
y <- mids_fwds.data$points
fit <- lm(y ~ ., data = X)
fit
#set number of folds
k <- 10
#use second filtered data set
data <- mids_fwds.data
n <- nrow(data)
#set response and predictors
y_var <- c("Points")
X_vars_drops <- drop_vars
#create vector of fold rmses
lm_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(train_y ~ ., data = train_X)
#predict using test data
y_hat <- predict(model_cv, test_X)
#append RMSE of k-th fold to rmse vector
linear_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
test_idx
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_set
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_X
train_y <- train_set[ ,y_var]
names(data)
names(train_set)
train_y <- train_set[ ,y_var]
#set response and predictors
y_var <- "Points"
train_y <- train_set[ ,y_var]
#set response and predictors
y_var <- c("points")
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(train_y ~ ., data = train_X)
#predict using test data
y_hat <- predict(model_cv, test_X)
#predict using test data
y_hat <- predict(linear_model_cv, test_X)
#append RMSE of k-th fold to rmse vector
linear_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
#set number of folds
k <- 10
#use second filtered data set
data <- mids_fwds.data
n <- nrow(data)
#set response and predictors
y_var <- c("points")
X_vars_drops <- drop_vars
#create vector of fold rmses
linear_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(train_y ~ ., data = train_X)
#predict using test data
y_hat <- predict(linear_model_cv, test_X)
#append RMSE of k-th fold to rmse vector
linear_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#mean of 10-fold CV RMSEs
mean_linear_rmse_cv <- mean(linear_rmse_cv)
print(mean_linear_rmse_cv)
#summary output to show evidence of non-linearity in variable effects
summary(linear_model_cv)
plot(linear_model_cv)
names(train_X)
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + s(below_half) + s(influence) + s(creativity) +
s(threat) + s(position_forward) + s(is_home_game) + s(X3G_avg_points) +
s(X3G_sum_brace_or_more)
, data=train_set)
library(gam)
#set number of folds
k <- 10
#use second filtered data set
data <- mids_fwds.data
n <- nrow(data)
#set response and predictors
y_var <- c("points")
X_vars_drops <- drop_vars
#create vector of fold rmses
gam_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + s(below_half) + s(influence) + s(creativity) +
s(threat) + s(position_forward) + s(is_home_game) + s(X3G_avg_points) +
s(X3G_sum_brace_or_more)
, data=train_set)
#predict using test data
y_hat <- predict(model_cv, test_X)
#append RMSE of k-th fold to rmse vector
gam_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + s(below_half) + s(influence) + s(creativity) +
s(threat) + s(position_Forward) + s(is_home_game) + s(X3G_avg_points) +
s(X3G_sum_brace_or_more)
, data=train_set)
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + f(below_half) + s(influence) + s(creativity) +
s(threat) + f(position_Forward) + f(is_home_game) + s(X3G_avg_points) +
s(X3G_sum_brace_or_more)
, data=train_set)
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + below_half + s(influence) + s(creativity) +
s(threat) + position_Forward + is_home_game + s(X3G_avg_points) +
s(X3G_sum_brace_or_more)
, data=train_set)
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + below_half + s(influence) + s(creativity) +
s(threat) + position_Forward + is_home_game + s(X3G_avg_points) +
X3G_sum_brace_or_more
, data=train_set)
#predict using test data
y_hat <- predict(model_cv, test_X)
#create vector of fold rmses
gam_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
model_cv <- gam(train_y ~ s(qual_diff) + below_half + s(influence) + s(creativity) +
s(threat) + position_Forward + is_home_game + s(X3G_avg_points) +
X3G_sum_brace_or_more
, data=train_set)
#predict using test data
y_hat <- predict(model_cv, test_X)
#append RMSE of k-th fold to rmse vector
gam_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#mean of 10-fold CV RMSEs
mean_gam_rmse_cv <- mean(gam_rmse_cv)
print(mean_gam_rmse_cv)
#summary output to show evidence of non-linearity in variable effects
summary(model_cv)
#plot smoothing function of variables from GAM
plot(model_cv, se = TRUE)
plot(linear_model_cv)
linear_model_cv <- lm(log(train_y) ~ ., data = train_X)
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 0.00001) ~ ., data = train_X)
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 0.001) ~ ., data = train_X)
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 0.1) ~ ., data = train_X)
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 1) ~ ., data = train_X)
train_y
log(train_y)
log(train_y+1)
log(train_y+1.1)
is.na(log(train_y+1.1))
sum(is.na(log(train_y+1.1)))
sum(is.na(log(train_y+10)))
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 4) ~ ., data = train_X)
#create vector of fold rmses
linear_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(log(train_y + 4) ~ ., data = train_X)
#predict using test data
y_hat <- predict(linear_model_cv, test_X)
#append RMSE of k-th fold to rmse vector
linear_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#mean of 10-fold CV RMSEs
mean_linear_rmse_cv <- mean(linear_rmse_cv)
print(mean_linear_rmse_cv)
#summary output to show evidence of non-linearity in variable effects
summary(linear_model_cv)
plot(linear_model_cv)
#create vector of fold rmses
linear_rmse_cv <- rep(0, k)
#set vector of fold indexes, specifies which observations belong to each fold
fold_idx <- sample(cut(1:n, breaks = k, labels = FALSE))
#loop through each fold
for (j in 1:k) {
#get the k-th fold for test data
test_idx <- which(fold_idx == j)
#data not in k-th fold becomes training set
train_set <- data[-test_idx, ]
train_X <- train_set[ ,!names(train_set) %in% X_vars_drops]
train_y <- train_set[ ,y_var]
#set test set
test_set <- data[test_idx, ]
#specify test predictor and y data
test_X <- test_set[ ,!names(test_set) %in% X_vars_drops]
test_y <- test_set[ ,y_var]
#fit gam without specifying spline degrees of freedome
linear_model_cv <- lm(train_y ~ ., data = train_X)
#predict using test data
y_hat <- predict(linear_model_cv, test_X)
#append RMSE of k-th fold to rmse vector
linear_rmse_cv[j] <- sqrt(mean((test_y - y_hat)^2))
}
#mean of 10-fold CV RMSEs
mean_linear_rmse_cv <- mean(linear_rmse_cv)
print(mean_linear_rmse_cv)
print(mean_gam_rmse_cv)
